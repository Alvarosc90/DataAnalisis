# -*- coding: utf-8 -*-
"""TP FlNAL,  Alvaro R. Soria Casali.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qjz-ezZpTZS3gTRjTNJkU9MVl7vsHxQT

# MODULO 1
"""

#IMPORTACION DE LIBRERIAS

import pandas as pd
import io
import os
import numpy as np
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

"""DATASET ELEGIDO
`IMPORTAMOS DESDE LA PAGINA OPEN ML`
"""

!pip install openml
import openml

ds = openml.datasets.get_dataset(185)
ds

#informacion del dataset elegido
print(ds.description)

#get data
x, y, categorical_indicator  , attribute_name = ds.get_data(
    dataset_format='dataframe', target=ds.default_target_attribute
                                                                 )

#atributos
attribute_name

#indicador de categorias
categorical_indicator

# Valores de X
x

#Valores de Y
y
# son los jugadores que se encuentran en el Hall de la fama

#concatenamos x y y
ds_1 = pd.concat([x, y], axis=1 )
ds_1

#LIMPIEZA DE LOS DATOS
# Vemos la cantidad de nulos:
ds_1.isnull().sum()
# Los datos nulos se refieren a aquellos jugadores que nunca tuvieron 3 strikes, por el momento dejo la base de datos como esta.

# Tamaño data set:
ds_1.shape
print('El dataset tiene {} instancias y {} Columnas'.format(ds_1.shape[0],ds_1.shape[1]))

"""VISUALIZACIÓN"""

# Importamos la librería Seaborn
import seaborn as sns

# Scatterplot:
# tamaño de figura
fig = plt.figure(figsize=(12, 8))

ax = sns.regplot(x="Hits", y="Games_played", data=ds_1)
ax.set(xlabel='Numero de Hits', ylabel='Cantidad de partidos jugados')
plt.title('Cantidad de juegos totales')
plt.show()

#Cantidad de posiciones de los jugadores
ds_1['Position'].value_counts()
#El Outfield cuenta con la mayor cantidad de jugadores, juega en una de las tres posiciones defensivas en béisbol o softbol , ​​la más alejada del bateador.

#Cantidad de jugadores por posicion
sns.catplot(y ="Position", data = ds_1,kind ="count");

# Cantidad de Home Runs
fig = plt.figure(figsize=(10,8))
sns.histplot(data=ds_1,x='Home_runs')
plt.title('Cantidad de Home Runs realizados')
plt.show()

fig = plt.figure(figsize=(10,8))
sns.histplot(data=ds_1, x='Home_runs',hue='Position')
plt.title('Cantidad de Home Runs realizados por cada posicion')
plt.show()

fig = plt.figure(figsize=(10,8))
sns.boxplot(x="Games_played", y='Position', data=ds_1)
plt.title('Cantidad de juegos jugados por posicion')
plt.show()

# joinplot de jugadores por posicion y juegos jugados
sns.jointplot(x="Games_played", y='Position', data=ds_1)

# Scatterplot de la relación entre la cantidad de juegos jugados y strikesouts recibidos
sns.relplot(x="Games_played", y='Strikeouts', data=ds_1,kind="scatter");

# Scatterplot de la relación entre la cantidad de juegos jugados y strikesouts recibidos diferenciados por posicion
sns.relplot(x="Games_played", y='Strikeouts', data=ds_1,kind="scatter", hue="Position");

# Scatterplot de la relación entre la cantidad de juegos jugados y Hits( Golpear a la bola) realizados diferenciados por aquellos que estan en el hall de la fama
sns.relplot(x="Games_played", y='Hits', data=ds_1,kind="scatter", hue="Hall_of_Fame");

import plotly.express as px

# Cantidad de juegos jugados por cada posicion, diferenciados ademas por aquellos que se encuentran en el hall de la fama
fig = px.box(ds_1, x="Games_played", y="Position", color="Hall_of_Fame",title ="Jugadores en el hall de la fama, diferenciados por posicion y cantidad de juegos jugados")
fig.show()

"""# **MODULO 2**

*MEDIDAS DE TENDENCIA CENTRAL*
"""

import scipy
scipy.stats.describe(ds_1['Games_played'])

scipy.stats.hmean(ds_1['Games_played']) # Media armónica

scipy.stats.gmean(ds_1['Games_played']) # Media geométrica

scipy.stats.mode(ds_1['Games_played']) # moda Age

scipy.stats.variation(ds_1['Games_played']) # Calculamos el coeficiente de variacion

scipy.stats.iqr(ds_1['Games_played']) # Calculamos el IQR

scipy.stats.skew(ds_1['Games_played']) # Calculamos el CA  Asimetría (Skewness)

scipy.stats.kurtosis(ds_1['Games_played']) # Calculamos el CA_p  Kurtosis (Sesgo)

scipy.stats.sem(ds_1['Games_played']) # Calculamos el Error estandar

"""DISTRIBUCION NORMAL"""

from scipy import stats

mu , sigma = 0 ,1

datos = ds_1['Games_played']
plt.figure(figsize=(6,6))
plt.hist(datos, 40, color="b", density = False)
plt.xlabel('Games_played')
plt.ylabel('Jugador')
plt.title('Histograma Normal de la cantidad de juegos jugados por cada jugador.')
plt.show()
#Se puede observar que la variable Games_played sigue una distrubucion normal

np.std(datos) #el desvio Standar de nuestra normal

np.var(datos) # la varianza

np.mean(datos) #La media

"""*REGRESION LINEAL*"""

# Creamos datos en 2 variables:

x = np.array(ds_1['Runs'])
y = np.array(ds_1['RBIs'])

y

# Valores en forma de vector:
X = x.reshape((-1,1))
X

# Graficamos la relación numeros de Runs vs RBIs
#En béisbol, una carrera impulsada, abreviado RBIs, carrera empujada o carrera remolcada se otorga a un bateador cuando su equipo anota una carrera como resultado de la aparición al plato de ese bateador.
sns.regplot(x= X, y= y, fit_reg=False, label='Original',marker='.')
plt.xlabel('RBIs')
plt.ylabel('Runs')
plt.title('RBIs vs Runs')
plt.legend()
plt.show()

# Vector con unos:
ones = np.ones(shape=(len(X),1))
ones

# Agregamos el vector x
X = np.append(ones, X, axis= 1)
X

# Tamaño X
X.shape

# Utilizamos el metodo linalg para multiplicar los vectores:
X1=np.linalg.inv(X.T @ X) # Calcular el inverso (multiplicativo) de una matriz

X1

# Estimaciones de betha:
betha = X1 @ (X.T @ y)

# b0 y b1
betha.round(2)

# Creamos nuestras predicciones
preds = X @ betha
preds

sns.regplot(x= x, y= y, fit_reg=False, marker='.' , color='blue', label= 'Original')
plt.plot(x, preds, label= 'Modelo', color='red')
plt.grid()
plt.xlabel('RBIs')
plt.ylabel('Runs')
plt.title('Relación')
plt.legend()
plt.title('Relación entre RBIs y Runs')
plt.show()

# Creamos la SSR, SSD, SST
#calculamos la suma de cuadrados de regresion, total y desvio

y_mean = np.mean(y)
SSR = (preds - y_mean)**2
SSD = (y - preds)**2
SST = (y - y_mean)**2

SSR_sum = sum(SSR)
SSD_sum = sum(SSD)
SST_sum = sum(SST)

#suma cuadrado de regresion + suma total
#coeficiente de determinacion
R2 = SSR_sum / SST_sum
R2.round(3)

R = np.sqrt(R2) # Vemos el coeficiente de correlación
R.round(3)

"""*REGRESION LINEAL SCIKIT LEARN*"""

# Importamos el modelo Linear Regression:
from sklearn.linear_model import LinearRegression

# Instanciamos el modelo
model = LinearRegression()

x

# fiteamos el modelo:
model.fit(X,y)

# Solicitamos la predicción:
model.predict(X)

# Estimación de B1
print("b1: {:.3f}".format(model.coef_[1]))

# Estimación de B0
print("b0:  {:.3f}".format(model.intercept_))

from sklearn.metrics import r2_score

# Calculamos r2
print("R Squares:{:.3f}".format(r2_score(y,preds)))

error = y - preds

# Ploteamos la distribución de los errores para ver su comportamiento:
sns.distplot(error, color='Magenta');

"""# **MODULO 3**

NORMALIZACION en la variable categorica Position
"""

ds_1['Position'].value_counts()

# Convertimos las features categóricas en numéricas:
data1=pd.get_dummies(ds_1,columns=['Position'])
data1

"""Se agregaron las columnas correspondientes al final de la base de datos, muchas veces puede resultar conveniente la normalizacion de algunas variables categoricas para su posterior analisis

**One Hot Encoding**
"""

from sklearn.preprocessing import OneHotEncoder

# Creamos la instancia OneHotEncoder:
enc = OneHotEncoder()

ds_1.info()

# Aplicamos el OneHotEncoder:
data2=pd.DataFrame(enc.fit_transform(ds_1[['Position']]).toarray())
data2

# Join con los df:
data3 = ds_1.join(data2)
data3

"""# **Estandarización**

Aplico estandarizacion para una variable para probar el metodo, mas adelante estandarizo a toda la base de datos para aplicar PCA
"""

datos_z = ds_1
datos_z['Games_played'] = (ds_1['Games_played'] - ds_1['Games_played'].mean()) / ds_1['Games_played'].std()

datos_z['Games_played']

# Histograma de los z score
datos_z['Games_played'].hist(figsize =(10, 10));

"""
# **Valores atípico - Outliers**
vamos a intentar observar aquellos jugadores que tuvieron una carrera mas larga que otros, y aquellos que jugaron mas temporadas de lo usual"""

plt.figure(figsize = (6, 4))
ds_1[['Games_played']].boxplot()
plt.show()

plt.figure(figsize = (5, 3))
ds_1[['Number_seasons']].boxplot()
plt.show()

#Estandarizo los valores
datos_z['Number_seasons1'] = (datos_z['Number_seasons'] - datos_z['Number_seasons'].mean()) / datos_z['Number_seasons'].std()
datos_z['Games_played1'] = (datos_z['Games_played'] - datos_z['Games_played'].mean()) / datos_z['Games_played'].std()

# Volvemos a realizar el Boxplot para analizar la distribución ya estandarizada:
plt.figure(figsize = (10, 7))
datos_z[['Games_played1', 'Number_seasons1']].boxplot()
plt.show()

datos_z[['Games_played1', 'Number_seasons1']].describe

# Aplicamos una funcion lambda para definir un rango:
datos_z['Games Duration_x']= datos_z['Games_played1'].apply(lambda x: 'Atipico' if (x>2 or x<-2) else 'Normal')
datos_z
datos_z['Number_seasons1_x']= datos_z['Number_seasons1'].apply(lambda x: 'Atipico' if (x>2 or x<-2) else 'Normal')
datos_z

datos_z['Games Duration_x'].value_counts()

datos_z['Number_seasons1_x'].value_counts()

import plotly.express as px

# Scatter para visualizar atípicos
fig = px.scatter(datos_z, x="Games_played1", y="Number_seasons1", color='Games Duration_x')
fig.show()

"""Hay varios valores atipicos, muy cerca de valores normales, por lo que podriamos deducir que dichos valores no son muy atipicos realmente.

### **Algoritmo Elliptic Envelope**
"""

from scipy import stats
from sklearn.covariance import EllipticEnvelope

df = datos_z [['Games_played1','Number_seasons1']]
df.head()

## Instanciamos la clase EllipticEnvelope a un objeto
outlier_method = EllipticEnvelope().fit(df)

#La siguiente función puntúa la normalidad de una instancia con respecto a la distribución centrada y escalada en la mediana e IQR, respectivamente
scores_pred = outlier_method.decision_function(df)

# se establece un umbral o frontera al percentil 25 %, el cual permite decidir si una muestra es o no outlier
threshold = stats.scoreatpercentile(scores_pred,25)

## ratio de outliers:
print(" %.2f %% " % (100*len(scores_pred[scores_pred < threshold]) / len(scores_pred)))

def grafico_outliers(df, outlier_method, outliers_begin, threshold, xmin, xmax):
    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(xmin, xmax, 100))
    Z = outlier_method.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(27, 21))
    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, xmax), cmap=plt.cm.Blues_r)
    a = plt.contour(xx, yy, Z, levels=[threshold], linewidths=2, colors='red')
    plt.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='orange')
    b = plt.scatter(df.iloc[:outliers_begin, 0], df.iloc[:outliers_begin, 1], c='white', s=20, edgecolor='k')
    c = plt.scatter(df.iloc[outliers_begin:, 0], df.iloc[outliers_begin:, 1], c='black', s=20, edgecolor='k')
    plt.axis('tight')
    plt.legend(
        [a.collections[0], b, c],
        ['Frontera de decisión', 'Instancias normales', 'Outliers'],
        prop=matplotlib.font_manager.FontProperties(size=34),
        loc='lower right')
    plt.show()

import matplotlib
import matplotlib.pyplot as plt

## Graficar outliers:
grafico_outliers(df, outlier_method, 150,threshold ,-6,6)

## Puntuaciones outliers:
scores_pred

## umbral de outliers:
threshold

"""#**Análisis de componentes principales**"""

datos_z

datos_z2 = datos_z.drop(['Position','Games Duration_x','Number_seasons1_x','Hall_of_Fame'], axis =1)

datos_z2.isnull().sum()

"""Al tener valores nulos procedemos a imputar dichos valores de Strikeouts"""

#PROCEDEMOS A IMPUTAR LOS VALORES DE Strikeouts
datos_z2['Strikeouts'].median()

#Imputamoss los valores vacíos de "Strikeouts" con el valor de la mediana
datos_z2['Strikeouts'] = datos_z2['Strikeouts'].fillna(datos_z2['Strikeouts'].median())

datos_z2.isnull().sum()

# Vemos el dataset:
datos_z2.head()

#Normalizamos los datos para que técnicas como PCA funcionen mejor!
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

stz = sc.fit_transform(datos_z2)
stz

import matplotlib.pyplot as plt

# Ploteamos la estandarización
plt.title("Plotting 1-D array")
plt.xlabel("X axis")
# plt.ylabel("Y axis")
plt.plot(stz, color = "blue", marker = "o")
plt.legend()
plt.show()

#Aplicación de PCA
from sklearn.decomposition import PCA

# Instanciamos:
pca = PCA(n_components=5)
datos_z_pca = pca.fit_transform(stz)

#Análisis de la varianza explicada para cada componente:
explained_variance = pca.explained_variance_ratio_
explained_variance



"""**Insights:**

El primer componente principal es responsable de la varianza del 64,02%. De manera similar, el segundo componente principal causa una variación del 10,96% en el conjunto de datos.
En conjunto, podemos decir que (64,02 + 10,96) el 74,98% por ciento de la información de clasificación contenida en el conjunto de características es capturada por los dos primeros componentes principales.
"""

# Porcentaje de varianza explicada por cada componente
# ==============================================================================
print('----------------------------------------------------')
print('Porcentaje de varianza explicada por cada componente')
print('----------------------------------------------------')
print(pca.explained_variance_ratio_)
print('')

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))
ax.bar(
    x      = np.arange(pca.n_components_) + 1,
    height = pca.explained_variance_ratio_
)

for x, y in zip(np.arange(len(datos_z2.columns)) + 1, pca.explained_variance_ratio_):
    label = round(y, 2)
    ax.annotate(
        label,
        (x,y),
        textcoords="offset points",
        xytext=(0,10),
        ha='center'
    )

ax.set_xticks(np.arange(pca.n_components_) + 1)
ax.set_ylim(0, 1.1)
ax.set_title('Porcentaje de varianza explicada por cada componente')
ax.set_xlabel('Componente principal')
ax.set_ylabel('Por. varianza explicada');

"""FIN MODULO 3

ALUMNO: SORIA CASALI, ALVARO RODRIGO FRANCO
"""